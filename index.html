<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>YOLOv8 Object Detection (Browser)</title>
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
  />
  <style>
    body {
      margin: 0;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
        sans-serif;
      background: #111;
      color: #fff;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: flex-start;
      height: 100vh;
    }

    h1 {
      font-size: 1.1rem;
      margin: 0.5rem;
      text-align: center;
    }

    #container {
      position: relative;
      width: 100%;
      max-width: 640px;
      flex: 1;
      display: flex;
      justify-content: center;
      align-items: center;
      overflow: hidden;
      background: #000;
    }

    video,
    canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: auto;
    }

    #overlay {
      pointer-events: none;
    }

    #controls {
      width: 100%;
      max-width: 640px;
      padding: 0.5rem;
      box-sizing: border-box;
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      justify-content: center;
      align-items: center;
      background: #181818;
      border-top: 1px solid #333;
    }

    button {
      padding: 0.5rem 0.9rem;
      border-radius: 999px;
      border: none;
      background: #1e88e5;
      color: white;
      font-size: 0.9rem;
      font-weight: 500;
      cursor: pointer;
    }

    button:disabled {
      opacity: 0.5;
      cursor: default;
    }

    #status {
      font-size: 0.8rem;
      color: #ccc;
      text-align: center;
      flex: 1 1 100%;
    }

    #preprocessCanvas {
      display: none; /* hidden; for resizing to model input */
    }
  </style>
</head>
<body>
  <h1>YOLOv8 Object Detection (Browser)</h1>

  <div id="container">
    <video id="video" autoplay playsinline muted></video>
    <canvas id="overlay"></canvas>
  </div>

  <div id="controls">
    <button id="startBtn" disabled>Start Camera</button>
    <button id="snapshotBtn" disabled>Capture Snapshot</button>
    <div id="status">Loading YOLO model…</div>
  </div>

  <!-- Hidden canvas used for 640x640 preprocessing -->
  <canvas id="preprocessCanvas"></canvas>

  <!-- ONNX Runtime for Web -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>

  <script>
    /**********************
     * CONFIG
     **********************/
    // YOLOv8n ONNX model hosted on Hugging Face (public, CORS-enabled)
    // If this ever changes, you can host your own yolov8n.onnx and update MODEL_URL.
    const MODEL_URL =
      "https://huggingface.co/SpotLab/YOLOv8Detection/resolve/3005c6751fb19cdeb6b10c066185908faf66a097/yolov8n.onnx?download=1";

    const INPUT_SIZE = 640;        // YOLOv8n input is 640x640
    const SCORE_THRESHOLD = 0.35;  // minimum confidence
    const IOU_THRESHOLD = 0.45;    // NMS IoU threshold
    const DETECTION_INTERVAL = 130; // ms between frames

    // COCO class labels (English)
    const COCO_CLASSES = [
      "person","bicycle","car","motorcycle","airplane","bus","train","truck","boat","traffic light",
      "fire hydrant","stop sign","parking meter","bench","bird","cat","dog","horse","sheep","cow",
      "elephant","bear","zebra","giraffe","backpack","umbrella","handbag","tie","suitcase",
      "frisbee","skis","snowboard","sports ball","kite","baseball bat","baseball glove","skateboard",
      "surfboard","tennis racket","bottle","wine glass","cup","fork","knife","spoon","bowl",
      "banana","apple","sandwich","orange","broccoli","carrot","hot dog","pizza","donut","cake",
      "chair","couch","potted plant","bed","dining table","toilet","tv","laptop","mouse","remote",
      "keyboard","cell phone","microwave","oven","toaster","sink","refrigerator","book","clock",
      "vase","scissors","teddy bear","hair drier","toothbrush"
    ];

    const COLORS = [
      "#ff5252","#4caf50","#40c4ff","#ffeb3b","#e040fb","#ff9800","#00e5ff","#9ccc65"
    ];

    /**********************
     * DOM ELEMENTS
     **********************/
    const video = document.getElementById("video");
    const overlay = document.getElementById("overlay");
    const ctx = overlay.getContext("2d");
    const preprocessCanvas = document.getElementById("preprocessCanvas");
    const preprocessCtx = preprocessCanvas.getContext("2d");
    const startBtn = document.getElementById("startBtn");
    const snapshotBtn = document.getElementById("snapshotBtn");
    const statusEl = document.getElementById("status");

    /**********************
     * STATE
     **********************/
    let session = null;
    let isDetecting = false;
    let lastDetectionTime = 0;

    /**********************
     * MODEL LOADING
     **********************/
    async function loadModel() {
      try {
        statusEl.textContent = "Loading YOLOv8n model (ONNX)…";
        session = await ort.InferenceSession.create(MODEL_URL, {
          executionProviders: ["wasm"],
        });
        statusEl.textContent = "Model loaded. Tap “Start Camera”.";
        startBtn.disabled = false;
      } catch (err) {
        console.error("Model load error:", err);
        statusEl.textContent = "Model load error: " + err.message;
      }
    }

    /**********************
     * CAMERA
     **********************/
    async function startCamera() {
      if (!session) {
        statusEl.textContent = "Model not ready yet.";
        return;
      }

      try {
        startBtn.disabled = true;

        const stream = await navigator.mediaDevices.getUserMedia({
          video: {
            facingMode: "environment",
            width: { ideal: 1280 },
            height: { ideal: 720 },
          },
          audio: false,
        });

        video.srcObject = stream;

        await new Promise((resolve) => {
          video.onloadedmetadata = () => {
            video.play();
            resolve();
          };
        });

        resizeCanvases();
        window.addEventListener("resize", resizeCanvases);

        isDetecting = true;
        snapshotBtn.disabled = false;
        statusEl.textContent = "Camera running. YOLOv8 detections in real time…";

        requestAnimationFrame(loop);
      } catch (err) {
        console.error("Camera error:", err);
        startBtn.disabled = false;
        statusEl.textContent = "Camera error: " + err.message;
      }
    }

    function resizeCanvases() {
      if (!video.videoWidth || !video.videoHeight) return;

      const aspectRatio = video.videoWidth / video.videoHeight;
      const container = document.getElementById("container");
      const containerWidth = container.clientWidth;
      const containerHeight = containerWidth / aspectRatio;

      video.width = containerWidth;
      video.height = containerHeight;
      overlay.width = containerWidth;
      overlay.height = containerHeight;
    }

    /**********************
     * PREPROCESSING
     * - Resize frame to 640x640
     * - Normalize to [0,1]
     * - Convert to NCHW float32
     **********************/
    function preprocessFrame() {
      preprocessCanvas.width = INPUT_SIZE;
      preprocessCanvas.height = INPUT_SIZE;

      // Draw current video frame into the 640x640 canvas
      preprocessCtx.drawImage(
        video,
        0,
        0,
        preprocessCanvas.width,
        preprocessCanvas.height
      );

      const imageData = preprocessCtx.getImageData(
        0,
        0,
        INPUT_SIZE,
        INPUT_SIZE
      );
      const { data } = imageData; // Uint8ClampedArray (RGBA)

      // Convert to Float32 [1, 3, 640, 640], normalized 0..1, RGB
      const floatData = new Float32Array(1 * 3 * INPUT_SIZE * INPUT_SIZE);
      let idx = 0; // index into floatData

      for (let y = 0; y < INPUT_SIZE; y++) {
        for (let x = 0; x < INPUT_SIZE; x++) {
          const i = (y * INPUT_SIZE + x) * 4;
          const r = data[i] / 255;
          const g = data[i + 1] / 255;
          const b = data[i + 2] / 255;

          // NCHW: [1, C, H, W]
          const offsetR = 0;
          const offsetG = INPUT_SIZE * INPUT_SIZE;
          const offsetB = 2 * INPUT_SIZE * INPUT_SIZE;

          const pixelIndex = y * INPUT_SIZE + x;

          floatData[offsetR + pixelIndex] = r;
          floatData[offsetG + pixelIndex] = g;
          floatData[offsetB + pixelIndex] = b;
        }
      }

      const inputTensor = new ort.Tensor("float32", floatData, [
        1,
        3,
        INPUT_SIZE,
        INPUT_SIZE,
      ]);

      return inputTensor;
    }

    /**********************
     * POSTPROCESSING
     * YOLOv8n ONNX typical output: [1, 84, N]
     *  - first 4: x, y, w, h (center format, relative to input)
     *  - remaining 80: class confidences
     **********************/
    function sigmoid(x) {
      return 1 / (1 + Math.exp(-x));
    }

    function xywh2xyxy(x, y, w, h) {
      // center x,y, width, height -> x1,y1,x2,y2
      const x1 = x - w / 2;
      const y1 = y - h / 2;
      const x2 = x + w / 2;
      const y2 = y + h / 2;
      return [x1, y1, x2, y2];
    }

    function iou(box1, box2) {
      const [x1, y1, x2, y2] = box1;
      const [x3, y3, x4, y4] = box2;

      const interX1 = Math.max(x1, x3);
      const interY1 = Math.max(y1, y3);
      const interX2 = Math.min(x2, x4);
      const interY2 = Math.min(y2, y4);

      const interArea = Math.max(interX2 - interX1, 0) * Math.max(interY2 - interY1, 0);
      const area1 = Math.max(x2 - x1, 0) * Math.max(y2 - y1, 0);
      const area2 = Math.max(x4 - x3, 0) * Math.max(y4 - y3, 0);

      const union = area1 + area2 - interArea;
      return union <= 0 ? 0 : interArea / union;
    }

    function nonMaxSuppression(boxes, scores, classIds, iouThreshold) {
      const indices = scores
        .map((score, idx) => ({ score, idx }))
        .sort((a, b) => b.score - a.score)
        .map((item) => item.idx);

      const picked = [];

      while (indices.length > 0) {
        const current = indices.shift();
        picked.push(current);

        const rest = [];
        for (const idx of indices) {
          if (classIds[idx] !== classIds[current]) {
            // keep different classes
            rest.push(idx);
            continue;
          }

          const iouVal = iou(boxes[current], boxes[idx]);
          if (iouVal < iouThreshold) {
            rest.push(idx);
          }
        }
        indices.splice(0, indices.length, ...rest);
      }

      return picked;
    }

    function processDetections(output, frameWidth, frameHeight) {
      // output: ort.Tensor, shape [1, 84, N]
      const [batch, channels, numBoxes] = output.dims;
      const data = output.data; // Float32Array length = 1*84*N

      const boxes = [];
      const scores = [];
      const classIds = [];

      for (let i = 0; i < numBoxes; i++) {
        const x = data[0 * numBoxes + i];
        const y = data[1 * numBoxes + i];
        const w = data[2 * numBoxes + i];
        const h = data[3 * numBoxes + i];

        let maxScore = -Infinity;
        let classId = -1;

        for (let c = 4; c < channels; c++) {
          const score = data[c * numBoxes + i];
          if (score > maxScore) {
            maxScore = score;
            classId = c - 4;
          }
        }

        const finalScore = sigmoid(maxScore); // YOLOv8 already applies sigmoid, but safe
        if (finalScore < SCORE_THRESHOLD) continue;
        if (classId < 0 || classId >= COCO_CLASSES.length) continue;

        const [x1, y1, x2, y2] = xywh2xyxy(x, y, w, h);

        // Scale from model-space (0..INPUT_SIZE) to frame-space
        const sx1 = (x1 / INPUT_SIZE) * frameWidth;
        const sy1 = (y1 / INPUT_SIZE) * frameHeight;
        const sx2 = (x2 / INPUT_SIZE) * frameWidth;
        const sy2 = (y2 / INPUT_SIZE) * frameHeight;

        boxes.push([sx1, sy1, sx2, sy2]);
        scores.push(finalScore);
        classIds.push(classId);
      }

      // Apply NMS
      const keep = nonMaxSuppression(boxes, scores, classIds, IOU_THRESHOLD);

      const results = keep.map((idx) => ({
        box: boxes[idx],
        score: scores[idx],
        classId: classIds[idx],
        label: COCO_CLASSES[classIds[idx]],
      }));

      return results;
    }

    /**********************
     * DRAWING
     **********************/
    function drawDetections(detections) {
      ctx.clearRect(0, 0, overlay.width, overlay.height);
      ctx.lineWidth = 2;
      ctx.font = "14px system-ui, sans-serif";

      detections.forEach((det, i) => {
        const [x1, y1, x2, y2] = det.box;
        const w = x2 - x1;
        const h = y2 - y1;
        const color = COLORS[i % COLORS.length];

        const labelText = `${det.label} ${(det.score * 100).toFixed(1)}%`;

        // Box
        ctx.strokeStyle = color;
        ctx.beginPath();
        ctx.rect(x1, y1, w, h);
        ctx.stroke();

        // Label background
        const textPadding = 4;
        const textWidth = ctx.measureText(labelText).width;
        const textHeight = 16;
        const bgX = x1;
        const bgY = Math.max(y1 - (textHeight + 4), 0);

        ctx.fillStyle = color;
        ctx.fillRect(bgX, bgY, textWidth + textPadding * 2, textHeight + 4);

        // Label text (English)
        ctx.fillStyle = "#000";
        ctx.fillText(
          labelText,
          bgX + textPadding,
          bgY + textHeight - 2
        );
      });
    }

    /**********************
     * MAIN LOOP
     **********************/
    async function detectOnce() {
      if (!session || !video.videoWidth || !video.videoHeight) return;

      const inputTensor = preprocessFrame();

      const feeds = {};
      const inputName = session.inputNames[0];
      feeds[inputName] = inputTensor;

      const results = await session.run(feeds);
      const outputName = session.outputNames[0];
      const output = results[outputName];

      const detections = processDetections(
        output,
        overlay.width,
        overlay.height
      );
      drawDetections(detections);
    }

    async function loop(timestamp) {
      if (!isDetecting) return;

      if (!lastDetectionTime || timestamp - lastDetectionTime > DETECTION_INTERVAL) {
        lastDetectionTime = timestamp;
        try {
          await detectOnce();
        } catch (err) {
          console.error("Detection error:", err);
          statusEl.textContent = "Detection error: " + err.message;
        }
      }

      requestAnimationFrame(loop);
    }

    /**********************
     * SNAPSHOT BUTTON
     * - Runs detection one time and keeps boxes on screen
     **********************/
    async function snapshotDetect() {
      if (!session || !video.videoWidth || !video.videoHeight) return;
      statusEl.textContent = "Running YOLO on snapshot…";
      try {
        await detectOnce();
        statusEl.textContent = "Snapshot detection done. Live detection still active.";
      } catch (err) {
        console.error("Snapshot detection error:", err);
        statusEl.textContent = "Snapshot error: " + err.message;
      }
    }

    /**********************
     * EVENT HOOKUP
     **********************/
    window.addEventListener("load", () => {
      loadModel();
    });

    startBtn.addEventListener("click", startCamera);
    snapshotBtn.addEventListener("click", snapshotDetect);
  </script>
</body>
</html>
